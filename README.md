TOPSIS-Based Ranking of Pre-Trained Conversational Models

Overview

This project utilizes the TOPSIS (Technique for Order Preference by Similarity to Ideal Solution) method to systematically rank pre-trained conversational AI models based on multiple performance criteria.

Dataset & Models

The following five pre-trained models are evaluated:
	â€¢	GPT-3.5
	â€¢	T5
	â€¢	XLNet
	â€¢	BERT
	â€¢	DialoGPT

Each model is assessed using the following criteria:
	â€¢	Accuracy (Higher is better)
	â€¢	Perplexity (Lower is better)
	â€¢	Response Time (Lower is better)
	â€¢	BLEU Score (Higher is better)
	â€¢	F1 Score (Higher is better)

Methodology

Normalization

Min-Max scaling is used to standardize values.

Weight Assignment

Each criterion is weighted based on significance.

TOPSIS Calculation
	â€¢	Compute ideal best and ideal worst values.
	â€¢	Determine Euclidean distances from ideal values.
	â€¢	Compute TOPSIS scores to rank models.

Installation & Usage

Prerequisites

Ensure Python and the following dependencies are installed:

pip install numpy pandas matplotlib scikit-learn seaborn

Execution

Run the script to compute model rankings and generate visualizations.

Results

The output ranks models based on TOPSIS scores. The final rankings are as follows:

Model	TOPSIS Score	Rank
GPT-3.5	0.679623	1
T5	0.657469	2
XLNet	0.404832	3
BERT	0.362986	4
DialoGPT	0.320377	5

Visualization Outputs
	â€¢	ðŸ“Š Bar Chart: Comparison of TOPSIS scores across models.
	â€¢	ðŸ¥§ Pie Chart: Score distribution breakdown.
	â€¢	ðŸ”¥ Heatmap: Weighted normalized decision matrix visualization.

Contributions

Feel free to contribute by:
	â€¢	Adding new pre-trained models
	â€¢	Modifying evaluation criteria
	â€¢	Optimizing the TOPSIS calculation
